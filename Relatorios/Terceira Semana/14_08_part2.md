# Relatório de Estudos

**Nome:** Lucas Lima Leite  
**Data:** 14/08/2024

## Módulos/Etapas  
1. [Linguagens e FrameWoks](#python)
    - [Python](#python)
        - [Características Principais](#características-principais)
        - [Introdução a python](#introdução-a-python-video)
    - [PySpark](#pyspark)
        - [PySparkSQL](#pysparksql)
        - [MLlib](#mllib)
        - [Características Principais](#características-principais-apache-spark)
        - [Exemplos de Aplicação](#exemplos-de-aplicação)
        - [Benefícios do PySPark](#benefícios-do-pyspark)
        - [Principais Comandos PySpark](#principais-comandos-pyspark)
4. [Considerações Finais](#recursos-utilizados)

## Python
Python é uma linguagem de programação de alto nível, interpretada de script, imperativa, orientada a objetos, funcional, de tipagem dinâmica e forte. Essa linguagem é comumente utilizada para:
* **Desenvolvimento Web -** Frameworks como Django e Flask são amplamente utilizados para construir sites e aplicações web robustas e escaláveis.

* **Data Science e Machine Learning -** Bibliotecas como NumPy, Pandas, Matplotlib, SciPy, Scikit-learn e TensorFlow fazem de Python uma escolha popular para análise de dados, visualização e machine learning.

* **Automação e Scripting -** Python é frequentemente utilizado para automatizar tarefas repetitivas, escrever scripts para gerenciamento de sistemas e automatização de processos.

* **Desenvolvimento de Software -** Python é utilizado para criar aplicações de software, desde pequenas ferramentas de linha de comando até grandes sistemas empresariais.

* **Inteligência Artificial e Deep Learning -** Frameworks como TensorFlow e PyTorch permitem o desenvolvimento de modelos de IA e deep learning, tornando Python uma das linguagens preferidas nesta área.

* **Computação Científica e Pesquisa -** Python é amplamente utilizado em pesquisa acadêmica e científica devido à sua capacidade de manipular e analisar grandes conjuntos de dados e sua integração com outras ferramentas científicas.

### Características Principais

* **Sintaxe Simples e Legível -** Python foi projetado para ser fácil de ler e escrever, o que a torna acessível para iniciantes e poderosa para desenvolvedores experientes.

* **Interpretação e Portabilidade -** Como uma linguagem interpretada, Python pode ser executada em várias plataformas, incluindo Windows, macOS e Linux, sem a necessidade de recompilação.

* **Bibliotecas e Frameworks Ricos -** Python possui uma vasta coleção de bibliotecas e frameworks, como NumPy, Pandas, Matplotlib, Django e Flask, que facilitam o desenvolvimento de diversas aplicações.

* **Comunidade Ativa -** A comunidade de desenvolvedores Python é grande e ativa, contribuindo continuamente com novos pacotes, recursos e suporte.

### [Introdução a Python (video)](https://www.youtube.com/watch?v=rfscVS0vtbw)

* Variáveis e tipos de dados.
* Strings.
* Numeros (**int, float**).
* Receber input do usuário.
* Listas e suas funções.
* Tuplas.
* Funções e funções com retorno.
* **if, else, elif**.
* Operadores de comparação.
* Dicionários.
* Loops com **while, for**.
* Listas em 2D (matrizes).
* **Try / Except** (tratamento de exceções).
* Trabalhando com arquivos (read, write, append, r+).
* bibliotecas externas.
* **BÁSICO** de orientação a objeto.


## PySpark
O **Apache Spark** é um framework conhecido por sua capacidade de processar grandes volumes de dados de forma distribuída e em memória, o que proporciona um desempenho significativamente mais rápido em comparação com outras tecnologias como o Hadoop MapReduce. O Apache Spark é comumente utilizado em áreas como, **análise de dados**, **machine learning**, **ETL(Extract, Transform and Load)**, **análise de bigdata**, **processamento de dados**.

**PySpark** é a interface do Apache Spark para a linguagem de programação Python, ela permite que os desenvolvedores utilizem a poderosa engine de processamento de dados do Spark usando a sintaxe e as bibliotecas do Python. A utilização do PySpark possui benefícios como:

* A capacidade de processar dados em memória e a otimização para cargas de trabalho iterativas, o que tornam o spark extremamente rápido.

* Suporte a diversas linguagens e uma API unificada para diferentes tipos de análise, permitindo uma ampla gama de aplicações.

* Com a utilização do PySpark, desenvolvedores podem aproveitar da simplicidade e da riqueza das bibliotecas de **Python**, combinando com o poder do spark.

### PySparkSQL

**PySparkSQL** é uma biblioteca PySPark para análises semelhante a SQL em grandes quantidades de dados estruturados e semi-estruturados, é possível usar o PySparkSQL para executar **queries SQL**, trabalhar com **Apache Hive** e até mesmo aplicar o HiveSQL. 

### MLlib

Essa biblioteca usa a técnica de paralelismo de dados para armazenar e trabalhar com dados, a MLlib é compatível com muitos algoritmos de machine learning para classificação, regressão, agrupamento, filtragem colaborativa, redução de dimensionalidade e primitivas de otimização subjacentes.

### Características Principais Apache Spark

* **Processamento em Memória -** Spark processa dados em memória, o que resulta em um desempenho muito mais rápido para workloads iterativas e interativas.

* **API Unificada para Diferentes Tipos de Análise -** Spark oferece APIs para processamento de dados estruturados (Spark SQL), streaming (Spark Streaming), machine learning (MLlib), e processamento de grafos (GraphX).

* **Escalabilidade e Distribuição -** Pode escalar facilmente de um único servidor para milhares de nós de computação, processando petabytes de dados.

* **Suporte a Várias Linguagens -** Spark suporta APIs em Python (PySpark), Java, Scala, e R, oferecendo flexibilidade aos desenvolvedores.

* **Facilidade de Integração -** Integra-se bem com outras ferramentas do ecossistema Big Data, como Hadoop, Hive, HBase, Cassandra e muitas outras.


### Exemplos de Aplicação

* **Processamento de Logs de Servidores -** Agregar e analisar grandes volumes de logs para monitoramento e detecção de anomalias.

* **Recomendações de Produtos -** Usar algoritmos de machine learning para recomendar produtos aos usuários com base em seu histórico de navegação e compras.

* **Análise de Dados de Sensores -** Processar dados de sensores em tempo real para manutenção preditiva e monitoramento de condições.

* **Análise de Dados de Redes Sociais -** Explorar e analisar dados de redes sociais para entender padrões de comportamento e tendências.

* **Análise Financeira -** Processar grandes volumes de dados financeiros para detecção de fraudes, análise de risco e previsão de mercado.

### Benefícios do PySpark
* **Desempenho -** A capacidade de processar dados em memória e a otimização para workloads iterativas tornam o Spark extremamente rápido.

* **Versatilidade -** Suporte a várias linguagens e uma API unificada para diferentes tipos de análise permitem uma ampla gama de aplicações.

* **Facilidade de Uso -** Com PySpark, desenvolvedores podem aproveitar a simplicidade e a riqueza de bibliotecas do Python, combinando-as com o poder do Spark.

* **Comunidade Ativa -** Sendo um projeto open-source, Spark tem uma comunidade ativa que contribui com novos recursos, melhorias e suporte.


### Principais comandos PySpark:

**Ler um arquivo CSV**
```python
df = spark.read.csv("caminho/do/arquivo.csv", header=True, inferSchema=True)
```
**Ler arquivos de um CSV a partir do MINIO**
```python
df = spark.read.option("header", "true").format("minioSelectCSV").csv("s3a://diretorio/nomedoarquivo.csv") 
```
**Ler um arquivo JSON**
```python
df =  spark.read.json('json_file.json')
```
**Mostrar o DataFrame**
```python
df.show()
```
**Select de colunas num DataFrame**
```python
df.select("col1", "col2").show()
```
**Filtragem de colunas num DataFrame (idade < 60 anos)**
```python
df.filter(df["age"] < "60").show()
```
**Contagem de dados iguais a partir agrupamento**
```python
df.groupBy("col1").count().show()
```
**Soma dos dados da col2 a partir do agrupamento da col1**
```python
df.groupBy("col1").agg(sum("col2")).show()
```
**Ordenando os dados da coluna em descendente**
```python
df.orderBy(Func.col("col1").desc()).show()
```
**Realizar um InnerJoin dos dois DataFrames**
```python
df1.join(df2, df1.ID == df2.ID).show()
```


### Recursos Utilizados: 
* [Introdução a Python](https://www.youtube.com/watch?v=rfscVS0vtbw) (Cursos de introdução a python, apenas relembrando o conhecimento que ja possuia).

* [Formação Spark com Pyspark (visualizado apenas parte do curso)](https://www.udemy.com/course/spark-curso-completo/learn/lecture/27991424#overview)

* Pesquisa em diversas fontes para obter maior compreensão dos conceitos dos tópicos do relatório.

### Desafios Encontrados:   
* **Python, PySpark -** Grande dificuldade de encontrar conteúdo prático da usabilidade do **PySpark** e do **Hive**.

### Feedback e Ajustes:
Seria de **GRANDE** ajuda se alguem mais experiente no uso do **PySpark** ou **Hive**, fizessem um breve **workshop** com um pouco do que acreditam que usaremos nesse futuro próximo onde iniaremos as atividades com a squad, comandos, conceitos basicos, etc. 

### Próximos Passos:  
Seguir com os estudos em **PySpark, Hive e Apache Airflow**.