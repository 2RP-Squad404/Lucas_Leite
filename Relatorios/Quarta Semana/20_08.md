# Relatório de Estudos

**Nome:** Lucas Lima Leite  
**Data:** 14/08/2024

## Módulos/Etapas  
1. [Linux/Shell](#linuxshell)
    - [Pincipais Características](#principais-características)
    - [Exemplos simples de Shell Script](#exemplos-simples-de-shell-script)
2. [Análise de Dados](#análise-de-dados)
    - [Conceitos de Análise de Dados](#conceitos-de-análise-de-dados)
    - [Ferramentas e Técnicas](#ferramentas-e-técnicas)
3. [Apache Hadoop](#apache-hadoop)
    - [Características Principais do Hadoop](#características-principais-do-hadoop)
    - [Desafios no uso do Hadoop](#desafios-no-uso-do-hadoop)
4. [Apache Hive](#apache-hive)
    - [Exemplo comandos Hive/SQL](#exemplo-comandos-hivesql)
5. [Apache Airflow](#apache-airflow)
    - [Caracteristicas Principais](#características-principais)
    - [Componentes Principais](#componentes-principaiss)
3. [Considerações Finais](#recursos-utilizados)

## Linux/Shell
Shell Script é um tipo de linguagem de script que é executador por um shell Unix ou Linux. Shell scripts são arquivos de texto que contêm uma série de comandos que o shell pode interpretar e executar, essa linguagem é utilizada para automatizar tarefas repetitivas e simplificar a execução de comandos complexos, gerando assim maior eficiência nos processos. Shell scripts são essenciais para a administração e configuração de sistemas Unix e Linux, eles permitem a criação de scripts de inicialização, gerenciamento de usuários, e configuração de redes.

### Principais Características
* **Shell -** O shell é um programa que fornece uma interface de linha de comando para interagir com o sistema operacional, como **Bash**, **Zsh** e **SH**.

* **Scripts de Sell -** Um script é um arquivo que contém uma sequência de comandos que são executados pelo shell, como ```ls``` para listar arquivos, ```grep``` para buscar padrões ou ```cp``` para copiar arquivos. Ele pode incluir variáveis, loops, condicionais e funções para executar tarefas automatizadas.

### Exemplos simples de Shell Script
1. **Script de Backup**:
    
    ```bash
    bashCopiar código
    #!/bin/bash
    tar -czf /backup/meuarquivo_backup_$(date +%F).tar.gz /meuarquivo
    
    ```
    
2. **Script de Verificação de Espaço em Disco**:
    
    ```bash
    bashCopiar código
    #!/bin/bash
    echo "Espaço disponível em disco:"
    df -h
    
    ```
    
3. **Script de Monitoramento de Processos**:
    
    ```bash
    bashCopiar código
    #!/bin/bash
    if pgrep "processo_exemplo" > /dev/null
    then
        echo "O processo está em execução."
    else
        echo "O processo não está em execução."
    fi
    
    ```

## Análise de dados
A análise de dados é o processo de inspecionar, limpar e modelar dados com o objetivo de descobrir informações úteis, tirar conclusões e apoiar a tomada de decisões, ela desempenha um papel crucial em diversas áreas, como negócios, ciência, engenharia e muito mais.

### Conceitos de Análise de Dados

* **Coleta de Dados -** O processo consiste em reunir dados de diferentes fontes.

* **Preparação e Limpeza dos Dados -** A etapa de preparar os dados para análise, o que inclui a limpeza de dados sujos, a eliminação de duplicatas, o tratamento de valores ausentes e a formatação de dados. 

* **Exploração dos Dados -** A análise preliminar dos dados para entender suas características principais, padrões e relações. O objetivo dessa etapa é obter uma visão geral dos dados, identifica tendências, padrões e anomalias que podem orientar a análise mais aprofundada.

* **Análise Descritiva -** A análise que busca descrever as características dos dados através de estatísticas e visualizações, inclui medidas como média, mediana, desvio padrão e frequência.

* **Análise Diagnóstica -** A análise que investiga o motivo pelo qual um evento ou tendência ocorreu. Geralmente envolve a análise de correlações e a identificação de causas potenciais.

* **Análise Preditiva -** Envolve o uso de técnicas estatísticas e algoritmos de machine learning para prever futuros eventos com base em dados históricos.

* **Análise Prescritiva -** A análise que sugere ações específicas para alcançar um resultado desejado. Tem como objetivo oferecer recomendações práticas sobre como melhorar processos, tomar decisões ou resolver problemas.

* **Visualização dos Dados -** A representação gráfica de dados para facilitar a compreensão e a interpretação. Inclui gráficos, tabelas, mapas e outras representações visuais.

* **Modelagem de Dados -** A construção de modelos matemáticos ou estatísticos para representar e analisar os dados. Inclui técnicas como regressão, clustering e análise de séries temporais.

* **Interpretação e Comunicação -** A etapa final de analisar e comunicar os insights obtidos dos dados. Envolve a interpretação dos resultados e a apresentação das descobertas para tomadores de decisão.

### Ferramentas e Técnicas
* **Ferramentas de Análise -** Softwares e plataformas como Excel, R, Python (pandas, numpy), SQL, Tableau, Power BI e Google Data Studio.

* **Técnicas Estatísticas -** Testes de hipóteses, análise de regressão, análise de variância, correlação e outras técnicas estatísticas.

* **Algoritmos de Machine Learning -** Regressão, árvores de decisão, redes neurais, clustering e outros.


## Apache Hadoop
O Hadoop é um framework de código aberto que permite o armazenamento e o processamento distribuído de grandes conjuntos de dados em clusteres, usando modelos de programação simples. Os principais benefícios do Hadoop são:  
**Escalonabilidade** por conta de seu modelo de computação distribuido no qual para aumentar processamento, pode apenas aumentar mais um nó.  
**Baixo Custo** pois pode ser executado em um hardware comum e possui um grande ecossistema de ferramentas.  
**Flexibilidade** pois os dados não exigem pré processamento antes de armazená-los, o que permite armazenar diversos dados para utilização futura.  
**Resiliência** por ser um modelo de computação distribuída, o Hadoop possui tolerância a falhas e resiliência do sistema, o que significa que, se um dos nós de hardware falhar, os jobs serão redirecionados para outros nós.

### Características Principais do Hadoop

* **HDFS (Hadoop Distributed File System) -** é um sistema de arquivos distribuído em que nós individuais do Hadoop operam em dados que residem no armazenamento local. Isso remove a latência da rede, fornecendo acesso de alta capacidade aos dados do aplicativo, além disso, os administradores não precisam definir esquemas antecipadamente.

* **YARN (Yet Another Resource Negotiator) -** o YARN é uma plataforma de gerenciamento de recursos responsável por gerenciar recursos de computação em clusters e usá-los para programar os aplicativos dos usuários. Ele realiza programação e alocação de recursos em todo o sistema Hadoop.

* **Map Reduce -**  o MapReduce é um modelo de programação para processamento de dados em grande escala. Nesse modelo, subconjuntos de conjuntos de dados maiores e instruções para processar os subconjuntos são enviados para vários nós diferentes, onde cada subconjunto é processado por um nó em paralelo com outros jobs de processamento. 

* **Hadoop Common -** o Hadoop Common inclui as bibliotecas e utilitários usados e compartilhados por outros módulos do Hadoop. 


### Desafios no uso do Hadoop
* **Mapeie a complexidade e as limitações -** Como um sistema com muitos arquivos, o MapReduce pode ser difícil de utilizar para trabalhos complexos, como tarefas analíticas interativas. O ecossistema do MapReduce é bastante grande, com muitos componentes para diferentes funções que podem dificultar a determinação das ferramentas a serem usadas.

* **Segurança -** Sensibilidade e proteção de dados podem ser problemas, já que o Hadoop lida com conjuntos de dados tão grandes.


## Apache Hive
O Apache Hive é um sistema de data warehouse para Apache Hadoop, que permite o resumo de dados, consultas e análise de dados. As Consultas de hive são escritas em HiveQL, que é uma linguagem de consulta semelhante ao SQL.

### Exemplo comandos Hive/SQL:

**Criar uma tabela vazia**
```SQL
CREATE TABLE table_name (data STRING, index int);
```

**Buscar todas as informações de uma tabela**
```SQL
SELECT * FROM table_name;
```

**Buscar uma coluna específica na tabela**
```SQL
SELECT index FROM table_name;
```

**Buscar uma coluna específica na tabela com uma condição**
```SQL
SELECT data FROM table_name WHERE index < 5;
```

**Buscar uma coluna específica na tabela com uma condição em ordem decrescente**
```SQL
SELECT data FROM table_name WHERE index < 5 ORDER BY index DESC;
```


## Apache Airflow
O Apache Airflow é uma ferramenta de orquestração open-source usada para criar, agendar e monitorar fluxos de trabalho complexos chamados DAGs(Directed Acyclic Graphs). O Airflow foi construído para lidar com dependências entre tarefas e para gerenciar falhas, fornecendo mecanismos de retentativa e notificações de alerta.  
As tarefas e suas dependências são definidas em Python, tornando-as mais flexíveis e acessíveis, além disso, o airflow possui uma interface de usuário baseada na web para vizualizar e gerenciar fluxos de trabalho, facilitando o monitoramento das pipelines.

### Características Principais 
* **Definição do fluxo de trabalho com Código -** Airflow usa Python para definir workflows, permitindo a criação de DAGs (Directed Acyclic Graphs) que descrevem a ordem e a lógica das tarefas.
* **Interface Web -** Oferece uma interface web rica para visualização, monitoramento e gerenciamento dos workflows e suas execuções.
* **Agendamento Flexível -** Suporta agendamentos de tarefas baseados em intervalos ou triggers, proporcionando flexibilidade na execução dos fluxos.
* **Execução Paralela e Escalabilidade -** Permite a execução paralela de tarefas e pode ser escalado para lidar com grandes volumes de tarefas e dados.
* **Retry e Resiliência -** Inclui mecanismos para retry automático de tarefas falhas e recuperação de erros, aumentando a robustez dos fluxos de trabalho.
* **Integração com Diversas Fontes -** Possui operadores e hooks para integração com uma ampla gama de sistemas e serviços, como bancos de dados, serviços de nuvem e APIs.

### Componentes Principais
* **DAG (Directed Acyclic Graph) -** Estrutura fundamental em Airflow que define o fluxo de tarefas, uma DAG é uma representação de como as tarefas se relacionam e a ordem em que devem ser executadas.
* **Tasks -** Unidades de trabalho definidas dentro de um DAG, ou seja, cada tarefa representa uma ação específica a ser executada.
* **Scheduler -** Componente que responsável por acionar as tarefas baseadas no cronograma definido e garantir que as tarefas sejam executadas conforme o esperado.
* **Executor -** Componente que lida com a execução real das tarefas. Pode ser configurado para usar diferentes backends, como Celery, Kubernetes, ou LocalExecutor.
* **Web Interface -** Interface gráfica para visualizar e gerenciar DAGs, monitorar o status das tarefas e visualizar logs de execução.

### Recursos Utilizados: 
* [The free hive book](https://github.com/Prokopp/the-free-hive-book/blob/master/the-free-hive-book.md#introduction)

* Pesquisa em diversas fontes para obter maior compreensão dos conceitos dos tópicos do relatório.

### Desafios Encontrados:   
* **Hive -** Grande dificuldade de encontrar conteúdos sobre **Hive**.

### Feedback e Ajustes:
Seria de **GRANDE** ajuda se alguem mais experiente no uso do **PySpark**,**Hive** ou **Airflow**, fizessem um breve **workshop** com um pouco do que acreditam que usaremos nesse futuro próximo onde iniciaremos as atividades com a squad, algo como comandos, conceitos basicos, etc. 

### Próximos Passos:  
Revisar grande parte do conteúdo e tentar usar as ferramentas citadas nesse relatório na prática.