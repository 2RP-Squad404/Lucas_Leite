# Relatório de Estudos

**Nome:** Lucas Lima Leite
**Data:** 07/08

**Módulos/Etapas Feitas:**  
1. **Big Data**
2. **Modelagem de Dados**

## Big Data

A definição de Big Data se da a partir de alguns conceitos conhecidos como V's. Inicialmente podia se defirnir através de 3 V's *principais*, porém hoje podemos encontrar conceitos mais desenvolvidos e expandidos com 5, 7 ou mais V's, sendo eles:
* **Volume:** Principal característica quando definimos Big Data, diz respeito a uma grande quantidade de dados para serem armazenados e processados (por volta dos terabytes, petabytes, ou maiores). A escala em que esses dados são acumulados imensa, ja que a cada momento mais dados são armazenados e processados.
* **Variedade:** É uma das 3 principais características de Big Data, na qual se um banco de dados deixa de trabalhar somente com dados *estruturados*, e passa a trabalhar tambem com dados *semi-estruturados*, e *não-estruturados*
    * **Estruturados:** São dados que seguem uma organização pré definida e esperada.
    * **Não Estruturados:** Eles não são estruturados em tabelas ou em  banco de dados relacionais, e podem conter diversos tipos de arquivos como texto, aúdio, vídeo, imagens, etc.
    * **Semi Estruturados:** São dados que não possuem uma estrutura pré definida, porém são organizados por meio de tags ou labels, permitindo agrupá-los e criar hierarquias. Esse tipo de estrutura também é conhecida como NoSQL.
* **Velocidade:** A última das características principais de Big Data aqui citadas, refere-se a velocidade em que os dados a serem armazendos são gerados. A todo momento dados como e-mails, mensagens de texto, audios são enviados, posts do instagram são publicados, registros em bancos de dados são inseridos e atualizados, e isso tudo de forma global. Além de dados de sensores que são enviados a todo instante.
* **Valor:** Consiste no aproveitamento em que uma organização pode ter em relação aos dados após o processo de coleta e análise.
* **Veracidade:** Se dá referente a qualidade e a confiábilidade dos dados.
* **Variabilidade:** É a medidade em que os pontos de dados em uma distribuição estatística ou conjunto de dados diferem do valor médio ou mediano, assim como esses pontos de dados variam entre si.
* **Visualização:** É a forma em que esses dados são apresentados, seja ele em forma de imagem, gráficos, documentos, etc. Caso bem feita, ela viabiliza que as tomadas de decisões de uma empresa sejam melhores, pois com uma boa apresentação de dados, a compreensão dos mesmos também seja melhor.

Ao longo do tempo, foram adotadas medidas para melhoramento dos hardwares de forma a que pudessem acompanhar o crescimento dos dados, e ainda sim manter os V's citados anteriomente. Métodos como a *Escalabilidade Vertical*, no qual adicionamos mais recursos como memória e processamento, passaram a não garantir uma grande efetividade ao se tratar de Big Data.

Para tratar dessas necessidades foram encontradas novas formas de realizar essa escalabilidade, sendo a principal delas o *Processamento Distribuido* ou *Escalabilidade Horizontal*. A ideia é utilizar de *cluster de máquinas*, de forma isolada um unico computador nesse cluster não possui poder de processamento ou armazenamento muito poderoso, mas, em conjunto, podem fornecer o necessário para suprir as necessidades.  
Nesse tipo de estrutura de máquinas, exite uma máquina principal *Name node* que reliza a gestão do restante das máquinas *Data nodes*. De forma a proteger esses dados, existe uma réplica desses dados em *Data nodes* diferentes, para que caso uma máquina venha a falhar, os dados não serão perdidos eestarão sempre disponíveis. O mais interessante desse tipo de estrutura é que caso se faça necessário aumentar as capacidade do cluster, novas máquinas podem ser adicionadas ao cluster, aumentando-o de forma indefinida.

### Ecossistema Hadoop
* **Hadoop:** É um framework que permite a distribuição de conjuntos de dados gigantes em um cluster de hardware comum.
* **Hive:** É um framework de ETL usada para consultar ou analisar grandes quantidades de dados armazenados num ecossistema hadoop. O *hive* possui três funções principais: resumo, query e análise de dados não estruturados e semi estruturados. Sua interface é semelhante a do SQL, e a linguagem HQL se comporta como SQL e traduz automaticamente as queries em jobs MapReduce.
* **Map Reduce:** É outra camada de processamento de dados no Hadoop. Possui a capacidade de processar grandes quantidades de dados estruturados e nao estruturados e pode dividir jobs em um conjunto de tarefas independentes para gerenciar arquivos de dados muito grandes em paralelo.
* **Apache Spark:** É um mecanismo de processamento de dados em memória, rápido e versátil. O Spark pode ser implementado de várias maneiras, e poss as linguagens de programção; *java*, *python*, *scala* e *R* e é compatível com SQL, streaming de dados, Machine Learning e processamento de dados.

### Tipos de Banco de Dados
* **MySQL** - Relacional.
* **PostgreSQL** - Relacional.
* **MongoDB** - NoSQL.
* **Cassandra** - NoSQL.


## Modelagem de Dados
A *Modelagem de Dados* visa garantir que os dados sejam organizados de maneira eficiente e que possam ser acessados e manipulados precisamente e consistentemente. Para que isso seja feito, esse modelo utiliza de representações visuais ou esquemas que definem os sistemas de coleta e gerenciamento de informações de qualquer organização, esses modelos ajudam os profissionais da área de dados (analistas, engenheiro, cientistas) a criar uma visão unificada dos dados de uma organização. O modelo *descreve:* quais dados a empresa coleta, a relçao entre os diferentes conjuntos de dados e os métodos que serão utilizados para armazenar e analisar esses dados. A modelagem de dados trás os seguintes benefícios:
* Faz com que os todos os stakeholders possam ter uma melhor compreensão dos dos dados e suas relações, e facilita a comunicação entre engenheiros de dados e equipes de BI (Business Intelligence).
* Reduz os erros e melhora a eficiência da desenvolvimento do banco de dados.
* Gera maior consistência na documentação dos dados do sistema de toda a organização.
* Torna mais fácil a manutenção e melhoria de sistemas de informação, permitindo ajustes e expansões de maneira mais controlada.

### Processos de Modelagem de Dados
1. **Levantamento de Requisitos:** Primeira etapa do processo na qual se realiza a identificação e documentação de todas as necessidades do negócio.
2. **Modelo Conceitual:** Utilizam de Diagramas ER, e outras ferramentas para representar entidades e relações, gerando uma visão geral dos dados, que explicam:
    * Quais dados o sistema contém.
    * Atributos de dados e condições ou restrições nos dados.
    * A quais regras de negócio os dados estão relacionados.
    * A melhor organização dos dados.
    * Requisitos de segurança e integridade dos dados.
3. **Modelo Lógico:** Eles fornecem maior detalhamento sobre os conceitos de dados e as relações identificadas no modelo de dados conceitual de forma mais técnica, mas ainda sem considerar implemetações fisicas, tal detalhamento se dá por:
    * Incluir os tipos de dados, atributos, e as relações entre entidades de forma mais *técnica*.
    * Atriutos primários e campos chave (PK, FK).
4. **Modelo Físico:** Converte um *modelo de dados lógico* em um banco de dados específico, incluindo detalhes sobre: 
    * Tabelas.
    * Índices.
    * Chaves Primárias (PK).
    * Chaves Estrangeiras (FK).
    * Técnicas de Armazenamento.
5. **Validação e Refinamento:** Etapa que verifica se o modelo atende aos requisitos e necessidades do negócio, e realiza os ajustes necessários para atende-los.
6. **Implementação:** Implementação do modelo físico validado e ajustado em um banco de dados e configuração dos dados de acordo com o modelo.

### Ferramentas de Modelagem de Dados
* **ER/Studio** 
* **IBM InfoSphere Data Architect** 
* **Microsoft Visio**
* **Oracle SQL Developer Data Modeler**
* **PowerDesigner**
* **MySQL Workbench**


**Recursos Utilizados:**  
- Utilizadas diversas fontes para melhor compreensão dos assuntos e tecnologias citadas na trilha de conhecimento.

**Principais comandos: (se aplicável)**  
- **Comandos SQL**  
    * `CREATE TABLE` - comando para criação de tabelas SQL.
    * `x int, FOREIGN KEY (x) REFERENCES y(x)`- criando FK a partir de uma PK existente.
    * `INSERT INTO x VALUES()` - insere dados na tabela *y*.
    * `UPDATE` - atualiza dados de uma tabela ja existente.
    * `SELECT` - realiza uma query em uma tabela em busca de dados específicos.
    * `INNER JOIN` - atributo utilizado dentro de uma query `SELECT` para realizar uma query com dados de duas ou mais tabelas desde que haja relação entre elas.
    * `NATURAL INNER JOIN` - atributo utilizado dentro de uma query `SELECT` para realizar uma query com dados de duas ou mais tabelas desde que haja relação entre elas, `NATURAL` busca uma realção natural entre as tabelas, algo como uma PK ou FK.
    * `WHERE` - onde, utilizado para definir onde será feita a ação anteriora a esse comando.
- **Comandos MongoDB** 
    * `db.x.find({campo: "valor"})` - comando para realizar uma query em um banco de dados MongoDB.
    * `db.x.find({campo: {$lt: 1950}})` - `$lt` comando para realizar uma busca no campo com valores abaixo de 1950 (não inclusivo).
    * `db.x.find({campo: {$gt: 1950}})` - `$gt` comando para realizar uma busca no campo com valores acima de 1950 (não inclusivo).
    * `db.x.find({$and: [{campo: {$gt: 199}, {campo: {$lt: 501}}]})` - `$and` comando para realizar uma busca com mais de uma condição, no código anterior, uma busca no campo *campo*, que seja maior que *199* e menor que *501*.
    * `db.x.find({}).sort({campo: -1}).limit(1)` - `.sort(campo:-1)` comando para realizar uma busca por elementos ordenando em decrescente. `.limit(1)` limita a quantidade de resultados ao valor em pareteses.
    * `db.x.find({campo: {$regex: "x", $options: 'i'}})` - `$regex` comando para buscar dentro do texto de um campo. `$options` comando que define como fazer a busca:  
    `i` define que busque CAIXA ALTA  e caixa baixa.   
    `x` ignora espaçoes em branco. 


**Desafios Encontrados:**  
**Big Data -** dificuldade de encontrar um conteúdo escrito de grande profundidade sobre assunto, que foi contornado pesquisando o conteúdo através de diersas fontes.
**Modelagem de dados -** Não possuí dificuldades com essa trilha.

**Feedback e Ajustes:**  
**Big Data -** acredito que poderia ter um pouco mais de profundidade no conteúdo e nas tecnologias comumente utilizadas.  
**Modelagem de dados -** Conteúdo do video de [Introdução a Modelagem de Dados](https://www.youtube.com/watch?v=SEnnucNP1h0&ab_channel=No-CodeStart-Up), um tanto quanto raso, mas faz bem o papel de *Introdução*, poderia ter um conteúdo mais aprofundado sobre modelos realcionais ou NoSQL.  
**Sugestão -** Com relação ao conteúdo da trilha e o aprendizado do pessoal, se possível seria interessante algo como ao final da semana pegar as dificuldades do pessoal, e na segunda feira ou terça logo depois, caso achem necessário, realizar um  breve workshop sanando as duvidas e dificuldades levantadas durante a semana anterior.

**Próximos Passos:**  
Descreva os próximos passos em sua trilha de aprendizagem. Quais são as próximas etapas ou módulos que você irá abordar?