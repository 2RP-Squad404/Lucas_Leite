# Relatório de Estudos

**Nome:** Lucas Lima Leite  
**Data:** 07/08

**Módulos/Etapas Feitas:**  
1. [Big Data](#big-data)
    - [Ecossistema Hadoop](#ecossistema-hadoop)
2. [Modelagem de Dados](#modelagem-de-dados)
    - [Processos de Modelagem de Dados](#processos-de-modelagem-de-dados)
    - [Ferramentas de Modelagem de Dados](#ferramentas-de-modelagem-de-dados)
3. [Analítico](#analítico)
    - [Tecnologias de Bases Analiticas](#tecnologias-de-bases-analíticas)
4. [Transacional](#transacional)
    - [Exemplos de Bases Transacionais](#exemplos-de-bases-transacionais)
    - [Benfícios](#benefícios)
    - [ACID](#acid)
5. [Considerações Finais](#desafios-encontrados)

## Big Data
A definição de Big Data se da a partir de alguns conceitos conhecidos como V's. Inicialmente podia se defirnir através de 3 V's *principais*, porém hoje podemos encontrar conceitos mais desenvolvidos e expandidos com 5, 7 ou mais V's, sendo eles:

* **Volume:** Principal característica quando definimos Big Data, diz respeito a uma grande quantidade de dados para serem armazenados e processados (por volta dos terabytes, petabytes, ou maiores). A escala em que esses dados são acumulados imensa, ja que a cada momento mais dados são armazenados e processados.
* **Variedade:** É uma das 3 principais características de Big Data, na qual se um banco de dados deixa de trabalhar somente com dados *estruturados*, e passa a trabalhar tambem com dados *semi-estruturados*, e *não-estruturados*
    * **Estruturados:** São dados que seguem uma organização pré definida e esperada.
    * **Não Estruturados:** Eles não são estruturados em tabelas ou em  banco de dados relacionais, e podem conter diversos tipos de arquivos como texto, aúdio, vídeo, imagens, etc.
    * **Semi Estruturados:** São dados que não possuem uma estrutura pré definida, porém são organizados por meio de tags ou labels, permitindo agrupá-los e criar hierarquias. Esse tipo de estrutura também é conhecida como NoSQL.

* **Velocidade:** A última das características principais de Big Data aqui citadas, refere-se a velocidade em que os dados a serem armazendos são gerados. A todo momento dados como e-mails, mensagens de texto, audios são enviados, posts do instagram são publicados, registros em bancos de dados são inseridos e atualizados, e isso tudo de forma global. Além de dados de sensores que são enviados a todo instante.

* **Valor:** Consiste no aproveitamento em que uma organização pode ter em relação aos dados após o processo de coleta e análise.

* **Veracidade:** Se dá referente a qualidade e a confiábilidade dos dados.

* **Variabilidade:** É a medidade em que os pontos de dados em uma distribuição estatística ou conjunto de dados diferem do valor médio ou mediano, assim como esses pontos de dados variam entre si.

* **Visualização:** É a forma em que esses dados são apresentados, seja ele em forma de imagem, gráficos, documentos, etc. Caso bem feita, ela viabiliza que as tomadas de decisões de uma empresa sejam melhores, pois com uma boa apresentação de dados, a compreensão dos mesmos também seja melhor.

Ao longo do tempo, foram adotadas medidas para melhoramento dos hardwares de forma a que pudessem acompanhar o crescimento dos dados, e ainda sim manter os V's citados anteriomente. Métodos como a *Escalabilidade Vertical*, no qual adicionamos mais recursos como memória e processamento, passaram a não garantir uma grande efetividade ao se tratar de Big Data.

Para tratar dessas necessidades foram encontradas novas formas de realizar essa escalabilidade, sendo a principal delas o *Processamento Distribuido* ou *Escalabilidade Horizontal*. A ideia é utilizar de *cluster de máquinas*, de forma isolada um unico computador nesse cluster não possui poder de processamento ou armazenamento muito poderoso, mas, em conjunto, podem fornecer o necessário para suprir as necessidades. 

Nesse tipo de estrutura de máquinas, exite uma máquina principal *Name node* que reliza a gestão do restante das máquinas *Data nodes*. De forma a proteger esses dados, existe uma réplica desses dados em *Data nodes* diferentes, para que caso uma máquina venha a falhar, os dados não serão perdidos eestarão sempre disponíveis. O mais interessante desse tipo de estrutura é que caso se faça necessário aumentar as capacidade do cluster, novas máquinas podem ser adicionadas ao cluster, aumentando-o de forma indefinida.

### Ecossistema Hadoop
* **Hadoop:** É um framework que permite a distribuição de conjuntos de dados gigantes em um cluster de hardware comum.
* **Hive:** É um framework de ETL usada para consultar ou analisar grandes quantidades de dados armazenados num ecossistema hadoop. O *hive* possui três funções principais: resumo, query e análise de dados não estruturados e semi estruturados. Sua interface é semelhante a do SQL, e a linguagem HQL se comporta como SQL e traduz automaticamente as queries em jobs MapReduce.

* **Map Reduce:** É outra camada de processamento de dados no Hadoop. Possui a capacidade de processar grandes quantidades de dados estruturados e nao estruturados e pode dividir jobs em um conjunto de tarefas independentes para gerenciar arquivos de dados muito grandes em paralelo.

* **Apache Spark:** É um mecanismo de processamento de dados em memória, rápido e versátil. O Spark pode ser implementado de várias maneiras, e poss as linguagens de programção; *java*, *python*, *scala* e *R* e é compatível com SQL, streaming de dados, Machine Learning e processamento de dados.

### Tipos de Banco de Dados
* **MySQL** - Relacional.
* **PostgreSQL** - Relacional.
* **MongoDB** - NoSQL.
* **Cassandra** - NoSQL.

## Modelagem de Dados
A *Modelagem de Dados* visa garantir que os dados sejam organizados de maneira eficiente e que possam ser acessados e manipulados precisamente e consistentemente. Para que isso seja feito, esse modelo utiliza de representações visuais ou esquemas que definem os sistemas de coleta e gerenciamento de informações de qualquer organização, esses modelos ajudam os profissionais da área de dados (analistas, engenheiro, cientistas) a criar uma visão unificada dos dados de uma organização. O modelo *descreve:* quais dados a empresa coleta, a relçao entre os diferentes conjuntos de dados e os métodos que serão utilizados para armazenar e analisar esses dados. A modelagem de dados trás os seguintes benefícios:

* Faz com que os todos os stakeholders possam ter uma melhor compreensão dos dos dados e suas relações, e facilita a comunicação entre engenheiros de dados e equipes de BI (Business Intelligence).

* Reduz os erros e melhora a eficiência da desenvolvimento do banco de dados.

* Gera maior consistência na documentação dos dados do sistema de toda a organização.

* Torna mais fácil a manutenção e melhoria de sistemas de informação, permitindo ajustes e expansões de maneira mais controlada.

### Processos de Modelagem de Dados
1. **Levantamento de Requisitos:** Primeira etapa do processo na qual se realiza a identificação e documentação de todas as necessidades do negócio.

2. **Modelo Conceitual:** Utilizam de Diagramas ER, e outras ferramentas para representar entidades e relações, gerando uma visão geral dos dados, que explicam:
    * Quais dados o sistema contém.
    * Atributos de dados e condições ou restrições nos dados.
    * A quais regras de negócio os dados estão relacionados.
    * A melhor organização dos dados.
    * Requisitos de segurança e integridade dos dados.
    
3. **Modelo Lógico:** Eles fornecem maior detalhamento sobre os conceitos de dados e as relações identificadas no modelo de dados conceitual de forma mais técnica, mas ainda sem considerar implemetações fisicas, tal detalhamento se dá por:
    * Incluir os tipos de dados, atributos, e as relações entre entidades de forma mais *técnica*.
    * Atriutos primários e campos chave (PK, FK)
    * 
4. **Modelo Físico:** Converte um *modelo de dados lógico* em um banco de dados específico, incluindo detalhes sobre: 
    * Tabelas.
    * Índices.
    * Chaves Primárias (PK).
    * Chaves Estrangeiras (FK).
    * Técnicas de Armazenamento.

5. **Validação e Refinamento:** Etapa que verifica se o modelo atende aos requisitos e necessidades do negócio, e realiza os ajustes necessários para atende-los.

6. **Implementação:** Implementação do modelo físico validado e ajustado em um banco de dados e configuração dos dados de acordo com o modelo.

### Ferramentas de Modelagem de Dados
* **ER/Studio** 
* **IBM InfoSphere Data Architect** 
* **Microsoft Visio**
* **Oracle SQL Developer Data Modeler**
* **PowerDesigner**
* **MySQL Workbench**

## Analítico
*Bases analiticas* referem-se a sistemas e tecnologias projetados para ter uma melhor analise de grandes volumes de dados, esses sistemas são otimizados para consultas complexas e operações analíticas nesses grandes volumes de dados e obtenham uma resposta/saída rapidamente, permitindo que as organizações obtenham insights valiosos de seus dados.  

 As principais caracteristicas desses sistemas vem de seu **desempenho em consultas**, o qual possui a capacidade de realizar consultas *complexas e pesadas* de forma *rápida e eficiente*, melhorando a eficiência das operações analíticas.  

 De sua **escalabilidade e flexibilidade** podendo lidar com grandes volumes de dados e aumentar a capacidade conforme necessário. 

 Da capacidade de **integração de dados** de multiplas fontes, sendo eles estruturados, semi estruturados ou não estruturados, proporcionando uma visão ampla das informações.  

 De sua **modelagem de dados** que utiliza de técnicas avançadas para organizar e relacionar informações de maneira lógica e acessível.

 De suas **ferramentas de análise** que incluim ferramentas para mineração de dados, machine learning, visualização de dados e análise estatística, ampliando as capacidades analíticas das organizações.
 Permitem a extração de **insights valiosos** dos dados, ajudando na tomada de decisões.

 ### Tecnologias de Bases Analíticas
* **OLAP(Online Analytical Processing) -** OLAP é um conceito de interface de usuário que proporciona uma análise de dados profunda em diversos ângulos. Essa tecnologia permite a realização de consultas multidimensionais sobre grandes volumes de dados, facilitando a análise complexa e exploração interativa dos dados.  
O OLAP possuí alguns tipos de operações como *Drill-down, Roll-up, Slice, Dice e Pivot*. Suas ferramentas e funções viabilizam sua utilização em áreas como BI, relatórios financeiros, análise de desempenho empresarial.

    * **Estruturas de Armazenamento:**
        * MOLAP(**M**ultidimensional Online Analytical Processing) - **Alto desempenho** e **baixa escalabilidade**.

        * ROLAP(**R**elacional Online Analytical Processing) - **Baixo desempenho** e **alta escalabilidade**.

        * HOLAP(**H**ybrid Online Analytical Processing) - É uma combinação de **MOLAP** e **ROLAP**, possuindo simultaneamente, um **bom desempenho** em consultas, e **boa escalabilidade**.
    * **Origem da Consulta:**
        * DOLAP(**D**esktop Online Analytical Processing) - A consulta é realizada de uma estação cliente diretamente a um servidor, o tráfego de rede é reduzido, melhorando o desempenho do servidor.

        * WOLAP(**W**eb Online Analytical Processing) - A consulta é realizada por meio de um navegador web a um servidor.
    * **Operações OLAP:**

        * *Drill-down* - É utilizado quando se necessita de informações mais detalhadas, sendo realizado através da *redução de granularidade*.

        * *Roll-up* - Funciona como o oposto ao *Drill-down*, utilizado quando se necessita de menos detalhes, havendo um *aumento da granularidade*.

        * *Slice* - Caracterizada pela *fixação de um valor para uma das dimensões,* obtendo assim, uma ***fatia do cubo de dados***.  

        * *Dice* - Realizada através da *seleção de dois ou mais valores das dimensões*, de modo a forma um **subcubo** de informações.

        * *Pivot* - Realiza a *rotação do cubo*, de modo a alterar a posição das dimensões, utilizado para apresentar as informações de forma alternativa.
* **Apache Spark -** É um mecanismo de análise unificado para processamento de dados em grande escala e velocidade, com módulos integrados para SQL, streaming, machine learning, processamento de gráficos e suporte a APIs em diversas linguagens (Java, Python, Scala e R). 

    O Spark pode ser executado no Hadoop, Kubernetes, por conta própria, na nuvem e em diversas fontes de dados, e oferece suporte a um processamento em tempo real e em batch.
    Isso viabiliza sua utilização em **análise de Big Data**, **machine learning**, **processamento de dados em tempo real** e **integração com data lakes e data warehouses**.

    * **Utilização do Apache Spark -** Muitas empresas estão usando o Spark para simplificar a tarefa de *processamento e análise** de grandes volumes de dados em **tempo real ou arquivados*, tanto *estruturados* quanto *não estruturados*. O spark também permite a integração de recursos complexos relevantes como machine learning e algorítmos de gráficos.
        * **Engenharia de dados -** Engenheiros de dados utilizam o Spark para para codificar e criar jobs de processamento de dados, com a opção de programar em um conjunto de idiomas expandido.
        * **Ciência de dados -** Cientistas de dados podem ter uma experiencia mais rica com análises e ML usando o Spark com GPUs. A capacidade de  processar volumer maiores de dados mais rápido, com uma linguagem familiar, pode ajudar a acelerar a inovação.

* **Google BigQuery -** O BigQuery é um data warehouse totalmente gerenciado da Google que ajuda a administrar e analisar dados com recursos integrados, como aprendizado de máquina, análise geoespacial e business intelligence. Suas ferramentas permitem realizar consultas analíticas em larga escala com grande rapidez.  

    A ferramenta possui total suporte a SQL, grande capacidade de processamento em tempo real, modelo de preços baseados em uso, além de integração nativa com outras ferramentas do GCP.  
    A arquitetura do BigQuery consiste em duas partes:
    * Uma camada de **armazenamento** que recebe, armazenda e otimiza dados.
    * E outra camada de **computação** que fornece recursos para análise.  

    Diferente dos banco de dados legados que geralmente precisam compartilhar recursos para operações de leitura/gravação e analiticas, que podem resultar em conflitos e tornar as consultas mais lentas enquanto os dados são gravados ou lidos a partir do armazenamento. 
    
    Com a **separação de recursos em duas camadas** de **computação e armazenamento** do BigQuery, cada uma delas pode *alocar recursos dinamicamente* sem afetar o *desempenho ou a disponibilidade* da outra.

    * **Armazenamento do BigQuery -** O BigQuery apresenta dados em tabelas, linhas e colunas e fornece suporte completo à semântica de transações de banco de dados (ACID). O armazenamento do BigQuery é replicado automaticamente em vários locais para proporcionar alta disponibilidade.

* **Amazon Redshift -** Utiliza SQL para analisar dados estruturados e semiestruturados em data warehouses, bancos de dados operacionais e data lakes, usando hardware e machine learning criados pela AWS para oferecer a melhor relação entre preço e performance em qualquer escala. As principais características de utilização do Redshisft, são:
    * **Escalabilidade**,

    * **Alta performance**, 

    * **Integração com o ecossistema AWS**,

    * **Suporte a consultas complexas**.

Essas qualidades viabilizam sua utilização para **BI**, **relatórios financeiros**, **análise de dados históricos** e **integração com ferramentas de visualização de dados**.

## Transacional
Bases transacionais ou OLTP(Online Transacntion Processing) são sistemas de gerenciamento de banco de dados projetados para gerenciar e facilitar o processamento de transações em tempo real. Essas transações são operações que envolvem a inserção, atualização, exclusao e consulta de dados e são comumente usadas em aplicativos que exigem um alto nivel de integridade e consistência dos dados. Como exemplos de bases transacionais, podemos citar:

### Exemplos de Bases Transacionais

* **MySQL -** Um sistema de gerenciamento de *banco de dados relacional* de código aberto amplamente utilizado para aplicações WEB e OLTP. Esse sistema oferece **suporte a SQL**, transações **ACID**, replicação e suporte a diversos mecanismos de de armazenamento, e geralmente é utilizado para aplicações WEB, e-commerce, sistemas de gerenciamento de conteudo.

* **PostgreSQL -** Um sistema de gerenciamento de banco de dados relacional de código aberto conhecido por sua *robustez* e *conformidade com padrões SQL*. Esse sistema oferece suporte a operações **ACID**, extensibilidade, capacidade de trabalhar com tipos de dados mais avançados e **suporte a JSON**, sua aplicação é comumente relacionada a aplicações empresariais, sistemas de BI, aplicações de geolocalização.

* **Oracle Database -** Um sistema de gerenciamento de banco de dados relacional proprietário amplamente utilizado em ambientes empresariais. Esse sistema oferece suporte a transações **ACID**, alta disponibilidade, recuperação de disastres através de funções para restaurar e recuperar o banco de dados e recursos avançados de segurança, sua aplicação é comumente relacionada a sistemas financeiros, ERPs(Planejamentos de Recursos Empresariais)

Essas bases transacionais são comumente utilizadas em:
* **Bancos e Finanças -** Processamento de transações bancárias, gestão de contas, processamento de pagamentos.

* **E-commerce -** Gerenciamento de Pedidos, processamento de pagamentos, gestão de inventário.

* **Saúde -** Registros eletrônicos de saúde, agendamento de consultas, gestão de pacientes.

* **Telecomunicações -** Gestão de assinaturas, faturamento, processamento de chamadas.

### Benefícios
* **Integridade dos dados -** A arquitetura de bases transacionais foram projetadas para serem compatíveis com **ACID**, garantindo que as gravações no banco de dados sejam bem sucedidas ou falhem jutas, mantendo um alto nível de integridade dos dados.

* **Baixa Latência -** Como foram projetados para executar sistemas de produção, eles são muito eficientes em operações que precisam ser concluidas em milissegundos. Ao fazer análises em uma réplica transacional de um banco de dados em produção, sua sincronização será muito próxima de atualizações em tempo real, ou seja, com latência de menos de um segundo.

* **Suporte a alta concorrencia -** Possui a capacidade de gerenciar muitos usuários simultanemamente sem comprometer a integridade dos dados.

* **Ecalabilidade -** Podem se escaladas para suportar grandes volumes de transações e dados.

### ACID
* **Atomicidade -** Seria a garantia de que a transação será feita totalmente ou não será feita. Nesse caso, a transação não é feita “pela metade”. Se por ventura uma operação da transação falhar, consequentemente, toda a  transação falhará.

* **Consistência -** Seria a proteção da integridade dos dados. Ou seja, se um banco de dados fizer uma operação que não seja válida, o processo será impedido e retornará para o estado inicial do processo.

* **Isolamento -** A capacidade de isolamento seria o fato de uma transação não “atrapalhar” a outra e ocorrer de forma isolada, garantindo que sejam feitas de forma individual, ou seja, transações que ainda não foram concluídas não podem ser modificadas por outras transações.

* **Durabilidade -** Seria a preservação dos dados após as operações terem sido realizadas. Ou seja, uma vez que uma transação for efetuada, ela permanecerá dessa forma, mesmo que ocorram problemas graves no sistema, sem precisar de retrabalho.

**Recursos Utilizados:**  
- Utilizadas diversas fontes para melhor compreensão dos assuntos e tecnologias citadas na trilha de conhecimento.

- Pesquisas em projetos passados, para exemplificar comandos SQL e MongoBB.

- [Conhecendo ao BigQuery](https://www.youtube.com/watch?v=fZkEDWTSfB0).

**Principais comandos: (se aplicável)**  
- **Comandos SQL**  
    * `CREATE TABLE` - comando para criação de tabelas SQL.
    * `x int, FOREIGN KEY (x) REFERENCES y(x)`- criando FK a partir de uma PK existente.
    * `INSERT INTO x VALUES()` - insere dados na tabela *y*.
    * `UPDATE` - atualiza dados de uma tabela ja existente.
    * `SELECT` - realiza uma query em uma tabela em busca de dados específicos.
    * `INNER JOIN` - atributo utilizado dentro de uma query `SELECT` para realizar uma query com dados de duas ou mais tabelas desde que haja relação entre elas.
    * `NATURAL INNER JOIN` - atributo utilizado dentro de uma query `SELECT` para realizar uma query com dados de duas ou mais tabelas desde que haja relação entre elas, `NATURAL` busca uma realção natural entre as tabelas, algo como uma PK ou FK.
    * `WHERE` - onde, utilizado para definir onde será feita a ação anteriora a esse comando.
- **Comandos MongoDB** 
    * `db.x.find({campo: "valor"})` - comando para realizar uma query em um banco de dados MongoDB.
    * `db.x.find({campo: {$lt: 1950}})` - `$lt` comando para realizar uma busca no campo com valores abaixo de 1950 (não inclusivo).
    * `db.x.find({campo: {$gt: 1950}})` - `$gt` comando para realizar uma busca no campo com valores acima de 1950 (não inclusivo).
    * `db.x.find({$and: [{campo: {$gt: 199}, {campo: {$lt: 501}}]})` - `$and` comando para realizar uma busca com mais de uma condição, no código anterior, uma busca no campo *campo*, que seja maior que *199* e menor que *501*.
    * `db.x.find({}).sort({campo: -1}).limit(1)` - `.sort(campo:-1)` comando para realizar uma busca por elementos ordenando em decrescente. `.limit(1)` limita a quantidade de resultados ao valor em pareteses.
    * `db.x.find({campo: {$regex: "x", $options: 'i'}})` - `$regex` comando para buscar dentro do texto de um campo. `$options` comando que define como fazer a busca:  
    `i` define que busque CAIXA ALTA  e caixa baixa.   
    `x` ignora espaçoes em branco. 

### Desafios Encontrados:
* **Big Data -** dificuldade de encontrar um conteúdo escrito de grande profundidade sobre assunto, que foi contornado pesquisando o conteúdo através de diersas fontes.  

* **Modelagem de dados -** Não possuí dificuldades com essa trilha.  

* **Analítico -** Dificil aprofundar em todas as tecnologias citadas, porém pudemos ter uma boa noção das bases de dados analíticas.

* **Transacional -** Sem grandes dificuldades para pontuar.


### Feedback e Ajustes:
* **Big Data -** acredito que poderia ter um pouco mais de profundidade no conteúdo e nas tecnologias comumente utilizadas.  

* **Modelagem de dados -** Conteúdo do video de [Introdução a Modelagem de Dados](https://www.youtube.com/watch?v=SEnnucNP1h0&ab_channel=No-CodeStart-Up), um tanto quanto raso, mas faz bem o papel de *Introdução*, poderia ter um conteúdo mais aprofundado sobre modelos realcionais ou NoSQL.  

* **Transacional -** Infelizmente não possuimos uma conta alura para realizar o curso de PostgreSQL disponivel na trilha.   

* **Sugestão -** Com relação ao conteúdo da trilha e o aprendizado do pessoal, se possível seria interessante algo como ao final da semana pegar as dificuldades do pessoal, e na segunda feira ou terça logo depois, caso achem necessário, realizar um  breve workshop sanando as duvidas e dificuldades levantadas durante a semana anterior.

### Próximos Passos: 
Como sequencia do conteúdo, devo seguir aprofundando no conteúdo de **Bigquery**, **linguagens** e **frameworks**. E caso eu consiga acesso a plataforma da udemy, pretendo fazer os cursos que acabei deixando de lado por não ter acesso, e fazer algo com maior foco em **python**, **hive**, **spark**.
